{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWqkdjGHAJdu"
      },
      "source": [
        "## Introduction to segmentation through deep learning\n",
        "\n",
        "## Tutorial 2. Lung segmentation in Chest X-Rays \n",
        "\n",
        "This lesson applies a [U-Net for Semantic Segmentation](https://arxiv.org/abs/1505.04597) of the lung fields on chest x-rays.\n",
        "\n",
        "### Download the dataset\n",
        "\n",
        "User can download many publicly available datasets like [Montgomery County dataset](https://lhncbc.nlm.nih.gov/LHC-publications/pubs/TuberculosisChestXrayImageDataSets.html), [JSRT dataset](http://db.jsrt.or.jp/eng.php) and [covid 19 dataset](https://github.com/v7labs/covid-19-xray-dataset) etc.  and train the lung segmentation model using those datasets. However due to time constraints, we will only train on montgomery dataset in this exercise.\n",
        "\n",
        "This tutorial and use of the public datasets are for education purpose only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Of0hEwAJd1"
      },
      "source": [
        "**Clink the badge above to launch this notebook on Google Colab.**\n",
        "To use GPU, go to Runtime -> Change runtime type -> and turn on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oAZF8PBl1gZ"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip install SimpleITK\n",
        "!pip install scikit-learn\n",
        "!pip install opencv-python\n",
        "!pip install pydicom\n",
        "!pip install segmentation_models\n",
        "!pip install scikit-image                                                                                                                                                                                                                                                                          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDyjDbOVAJeG"
      },
      "source": [
        "### Import all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import tensorflow as tf\n",
        "from skimage import transform, io, img_as_float, exposure\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D, concatenate"
      ],
      "metadata": {
        "id": "7WRzGfnTA_Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount your Google drive and allow the colab to access your Google drive "
      ],
      "metadata": {
        "id": "7wcMazPwQBNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6BXgl35l1gf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "root_dir = '/content/drive/MyDrive/niehs_tutorials'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHANkP0uAJeo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m64NObENAJe9"
      },
      "source": [
        "### Combine left and right lung masks from Montgomery County dataset and prepare load data function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_right_left_lung_masks(data_dir_path):\n",
        "    for filename in glob.glob(os.path.join(data_dir_path,'CXR_png','*.png')):\n",
        "        print('Combined right and left lung masks for '+filename) \n",
        "        left = io.imread(os.path.join(data_dir_path,'ManualMask','leftMask', os.path.basename(filename)))\n",
        "        right = io.imread(os.path.join(data_dir_path,'ManualMask','rightMask', os.path.basename(filename)))\n",
        "        io.imsave(os.path.join(data_dir_path,'combined_masks', os.path.basename(filename)), left+right)\n",
        "\n",
        "\n",
        "def load_data(data_dir_path,resize_shape):\n",
        "    #combine_right_left_lung_masks(data_dir_path)\n",
        "    cxr_filenames = glob.glob(os.path.join(data_dir_path,\"CXR_png\",'*.png'))  # CXR paths\n",
        "    masks_filenames = glob.glob(os.path.join(data_dir_path,\"combined_masks\",'*.png'))  # Masks paths\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for file in  cxr_filenames:\n",
        "          cxr = sitk.ReadImage(file)\n",
        "          mask = sitk.ReadImage(os.path.join(data_dir_path,\"combined_masks\",os.path.basename(file)))\n",
        "          new_spacing = [sz*spc/nsz for nsz,sz,spc in zip(resize_shape, cxr.GetSize(), cxr.GetSpacing())]\n",
        "\n",
        "          resampled_org = sitk.Resample(cxr, resize_shape, sitk.Transform(), sitk.sitkLinear,\n",
        "                                            cxr.GetOrigin(), new_spacing, cxr.GetDirection(),\n",
        "                                            0, sitk.sitkFloat32)\n",
        "          resampled_seg = sitk.Resample(mask, resize_shape, sitk.Transform(), sitk.sitkNearestNeighbor,\n",
        "                                            mask.GetOrigin(), new_spacing, mask.GetDirection(),\n",
        "                                            0, sitk.sitkInt32)\n",
        "          img = sitk.GetArrayFromImage(resampled_org)\n",
        "          seg = sitk.GetArrayFromImage(resampled_seg)\n",
        "          #Standardize image between [0,1]\n",
        "          img = (img - img.min()) / (img.max() - img.min())\n",
        "          img = exposure.equalize_adapthist(img)\n",
        "          img = np.expand_dims(img, axis=-1)\n",
        "          seg = seg/255\n",
        "          X.append(img)\n",
        "          Y.append(seg)\n",
        "\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    return X,Y"
      ],
      "metadata": {
        "id": "XW_VJU6NCJA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWSgeLUR_pvj"
      },
      "source": [
        "### Load data "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir_path = os.path.join(root_dir,'MontgomerySet')\n",
        "X,Y = load_data(data_dir_path,resize_shape=(256,256))"
      ],
      "metadata": {
        "id": "eyjEOP98Cbnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize data when right and left lung amsks are combined."
      ],
      "metadata": {
        "id": "z9I5-ZxyuMnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 5\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=10,10\n",
        "_,subfigs = plt.subplots(num_samples, 2)\n",
        "for sample in range(num_samples):\n",
        "      arr = X[sample,:,:,0]\n",
        "      ground_truth = Y[sample,:,:]\n",
        "      subfigs[sample,0].imshow(arr, cmap = 'gray')\n",
        "      subfigs[sample,0].set_title('Sample Image #'+str(sample))\n",
        "      subfigs[sample,0].axis('off')\n",
        "      subfigs[sample,1].imshow(ground_truth, cmap = 'gray')\n",
        "      subfigs[sample,1].set_title('Ground Truth #'+str(sample))\n",
        "      subfigs[sample,1].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CCU-2GTGuL4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Divide train/val/test sets for model development"
      ],
      "metadata": {
        "id": "UdmHhEFvuVql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train-val-test split \n",
        "random.Random(4).shuffle(X)\n",
        "random.Random(4).shuffle(Y)\n",
        "split_1 = int(0.8 * len(X))\n",
        "split_2 = int(0.9 * len(X))\n",
        "X_train,Y_train = X[:split_1],Y[:split_1]\n",
        "X_val,Y_val = X[split_1:split_2],Y[split_1:split_2]\n",
        "X_test,Y_test = X[split_2:],Y[split_2:]"
      ],
      "metadata": {
        "id": "zStkGCP9FaLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the model"
      ],
      "metadata": {
        "id": "pi5qYv8-DczV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def UNet(inp_shape, k_size=3):\n",
        "    merge_axis = -1 # Feature maps are concatenated along last axis (for tf backend)\n",
        "    data = Input(shape=inp_shape)\n",
        "    conv1 = Convolution2D(filters=32, kernel_size=k_size, padding='same', activation='relu')(data)\n",
        "    conv1 = Convolution2D(filters=32, kernel_size=k_size, padding='same', activation='relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(pool1)\n",
        "    conv2 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(pool2)\n",
        "    conv3 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(pool3)\n",
        "    conv4 = Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(pool4)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2, 2))(conv5)\n",
        "    conv6 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(up1)\n",
        "    conv6 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(conv6)\n",
        "    merged1 = concatenate([conv4, conv6], axis=merge_axis)\n",
        "    conv6 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(merged1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    conv7 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(up2)\n",
        "    conv7 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(conv7)\n",
        "    merged2 = concatenate([conv3, conv7], axis=merge_axis)\n",
        "    conv7 = Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(merged2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2, 2))(conv7)\n",
        "    conv8 = Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(up3)\n",
        "    conv8 = Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(conv8)\n",
        "    merged3 = concatenate([conv2, conv8], axis=merge_axis)\n",
        "    conv8 = Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(merged3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(conv8)\n",
        "    conv9 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(up4)\n",
        "    conv9 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv9)\n",
        "    merged4 = concatenate([conv1, conv9], axis=merge_axis)\n",
        "    conv9 = Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(merged4)\n",
        "\n",
        "    conv10 = Convolution2D(filters=1, kernel_size=k_size, padding='same', activation='sigmoid')(conv9)\n",
        "\n",
        "    output = conv10\n",
        "    model = Model(data, output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "EWeqrYA87aew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 8\n",
        "learning_rate = 0.0001 \n",
        "\n",
        "model = UNet((256,256,1), k_size=3)\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=learning_rate)  # ,momentum=.8,nesterov=True) #.00001\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'],\n",
        "                  )"
      ],
      "metadata": {
        "id": "xzGe4YQsDb-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "0cM8262pLzLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7RNocekl1gk"
      },
      "outputs": [],
      "source": [
        "# Callbacks to stop early and save model periodically\n",
        "model_output_filename = os.path.join(root_dir,'best_lung_segment_model.h5')\n",
        "earlystopper = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)\n",
        "modelSaver = ModelCheckpoint(model_output_filename, monitor='val_loss', verbose=0,\n",
        "                              save_best_only=True,\n",
        "                              save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val),\n",
        "              callbacks=[earlystopper, modelSaver])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "AAawGT0BCTxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define metrics\n",
        "\n",
        "# Intersection Over Union\n",
        "def IoU(y_true, y_pred):\n",
        "    \n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.logical_and(y_true_f, y_pred_f).sum()\n",
        "    union = np.logical_or(y_true_f, y_pred_f).sum()\n",
        "    return (intersection + 1) * 1. / (union + 1)\n",
        "\n",
        "# Dice coefficient\n",
        "def Dice(y_true, y_pred):\n",
        "\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.logical_and(y_true_f, y_pred_f).sum()\n",
        "    return (2. * intersection + 1.) / (y_true.sum() + y_pred.sum() + 1.)\n",
        "\n",
        "#Misclassification rate\n",
        "def misclassification_rate(y_true,y_pred):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    fn = tf.keras.metrics.FalseNegatives()\n",
        "    fn.update_state(y_true_f,y_pred_f)\n",
        "    fn = fn.result().numpy()\n",
        "    fp = tf.keras.metrics.FalsePositives()\n",
        "    fp.update_state(y_true_f,y_pred_f)\n",
        "    fp = fp.result().numpy()\n",
        "    tp = tf.keras.metrics.TruePositives()\n",
        "    tp.update_state(y_true_f,y_pred_f)\n",
        "    tp =tp.result().numpy()\n",
        "    tn = tf.keras.metrics.TrueNegatives()\n",
        "    tn.update_state(y_true_f,y_pred_f) \n",
        "    tn = tn.result().numpy()\n",
        "    misclassification_rate = (fp+fn)/(fp+fn+tp+tn)  \n",
        "    return misclassification_rate\n",
        "    \n"
      ],
      "metadata": {
        "id": "ZuzSJlwDDk7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_filename = os.path.join(root_dir,'best_lung_segment_model.h5')\n",
        "model = tf.keras.models.load_model(model_output_filename)\n",
        "preds = model.predict(X_test,batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Binarize masks with the threshold\n",
        "threshold=0.5\n",
        "preds = preds > threshold\n",
        "\n",
        "ious = np.zeros(len(X_test))\n",
        "dices= np.zeros(len(X_test))\n",
        "misclassify_rates = np.zeros(len(X_test))\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "     ious[i] = IoU(preds[i],Y_test[i])\n",
        "     dices[i] = Dice(preds[i],Y_test[i])\n",
        "     misclassify_rates[i] = misclassification_rate(preds[i],Y_test[i])\n",
        "\n",
        "print(\"Mean Dice:\"+str(dices.mean()))\n",
        "print(\"Mean IoU:\"+str(ious.mean()))\n",
        "print(\"Misclassification rate:\"+str(misclassify_rates.mean()))"
      ],
      "metadata": {
        "id": "MrbJvsxXLyqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the results"
      ],
      "metadata": {
        "id": "pX1qGmNqFeea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 5\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=10,10\n",
        "_,subfigs = plt.subplots(num_samples, 3)\n",
        "for sample in range(num_samples):\n",
        "      arr = X_test[sample,:,:,0]\n",
        "      ground_truth = Y_test[sample,:,:]\n",
        "      prediction = preds[sample,:,:,0].astype(int)\n",
        "      subfigs[sample,0].imshow(arr, cmap = 'gray')\n",
        "      subfigs[sample,0].set_title('Test Image #'+str(sample))\n",
        "      subfigs[sample,0].axis('off')\n",
        "      subfigs[sample,1].imshow(ground_truth, cmap = 'gray')\n",
        "      subfigs[sample,1].set_title('Ground Truth #'+str(sample))\n",
        "      subfigs[sample,1].axis('off')\n",
        "      subfigs[sample,2].imshow(prediction, cmap = 'gray')\n",
        "      subfigs[sample,2].set_title('Prediction #'+str(sample))\n",
        "      subfigs[sample,2].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5d48usJAFdn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now perform morphological operations to see if you can improve the overall performance"
      ],
      "metadata": {
        "id": "2ELngeh_Famr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dilate_prediction(prediction,radius=6):\n",
        "    sample = sitk.GetImageFromArray(prediction.astype(int))\n",
        "    dilateFilter = sitk.DilateObjectMorphologyImageFilter()\n",
        "    dilateFilter.SetKernelRadius(radius)\n",
        "    out = dilateFilter.Execute(sample)\n",
        "    out_arr = sitk.GetArrayFromImage(out)\n",
        "    return out_arr\n",
        "\n",
        "def erode_prediction(prediction,radius=6):\n",
        "    sample = sitk.GetImageFromArray(prediction.astype(int))\n",
        "    erodeFilter = sitk.BinaryErodeImageFilter()\n",
        "    erodeFilter.SetKernelRadius(radius)\n",
        "    out = erodeFilter.Execute(sample)\n",
        "    out_arr = sitk.GetArrayFromImage(out)\n",
        "    return out_arr\n",
        "\n",
        "def close_prediction(prediction,radius=6):\n",
        "    sample = sitk.GetImageFromArray(prediction.astype(int))\n",
        "    closingFilter = sitk.BinaryMorphologicalClosingImageFilter()\n",
        "    closingFilter.SetKernelRadius(radius)\n",
        "    closingFilter.SetForegroundValue(1)\n",
        "    out = closingFilter.Execute(sample)\n",
        "    out_arr = sitk.GetArrayFromImage(out)\n",
        "    return out_arr\n",
        "\n",
        "def open_prediction(prediction,radius=6):\n",
        "    sample = sitk.GetImageFromArray(prediction.astype(int))\n",
        "    openingFilter = sitk.BinaryMorphologicalOpeningImageFilter()\n",
        "    openingFilter.SetKernelRadius(radius)\n",
        "    openingFilter.SetForegroundValue(1)\n",
        "    out = openingFilter.Execute(sample)\n",
        "    out_arr = sitk.GetArrayFromImage(out)\n",
        "    return out_arr\n"
      ],
      "metadata": {
        "id": "sdSzqRcnq8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check performance of the model predictions when each of the morphological operations are performed. User canfeel free to change the kernel radius for each of the morphological operation fucntion and see how the performance varies"
      ],
      "metadata": {
        "id": "BmF9BD2ytclz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ious = np.zeros(len(X_test))\n",
        "dices= np.zeros(len(X_test))\n",
        "misclassify_rates = np.zeros(len(X_test))\n",
        "for i in range(len(X_test)):\n",
        "     ious[i] = IoU(dilate_prediction(preds[i],radius=1),Y_test[i])\n",
        "     dices[i] = Dice(dilate_prediction(preds[i],radius=1),Y_test[i])\n",
        "     misclassify_rates[i] = misclassification_rate(dilate_prediction(preds[i],radius=1),Y_test[i])\n",
        "\n",
        "print(\"Mean Dice when dilation filter is applied:\"+str(dices.mean()))\n",
        "print(\"Mean IoU when dilation filter is applied::\"+str(ious.mean()))\n",
        "print(\"Misclassification rate:\"+str(misclassify_rates.mean()))\n",
        "\n",
        "ious = np.zeros(len(X_test))\n",
        "dices= np.zeros(len(X_test))\n",
        "misclassify_rates = np.zeros(len(X_test))\n",
        "for i in range(len(X_test)):\n",
        "     ious[i] = IoU(erode_prediction(preds[i],radius=1),Y_test[i])\n",
        "     dices[i] = Dice(erode_prediction(preds[i],radius=1),Y_test[i])\n",
        "     misclassify_rates[i] = misclassification_rate(erode_prediction(preds[i],radius=1),Y_test[i])\n",
        "\n",
        "print(\"Mean Dice when erosion filter is applied:\"+str(dices.mean()))\n",
        "print(\"Mean IoU when erosion filter is applied::\"+str(ious.mean()))\n",
        "print(\"Misclassification rate:\"+str(misclassify_rates.mean()))\n",
        "\n",
        "ious = np.zeros(len(X_test))\n",
        "dices= np.zeros(len(X_test))\n",
        "misclassify_rates = np.zeros(len(X_test))\n",
        "for i in range(len(X_test)):\n",
        "     ious[i] = IoU(close_prediction(preds[i],radius=1),Y_test[i])\n",
        "     dices[i] = Dice(close_prediction(preds[i],radius=1),Y_test[i])\n",
        "     misclassify_rates[i] = misclassification_rate(close_prediction(preds[i],radius=1),Y_test[i])\n",
        "\n",
        "print(\"Mean Dice when morpholical closing filter is applied:\"+str(dices.mean()))\n",
        "print(\"Mean IoU when morpholical closing filter is applied::\"+str(ious.mean()))\n",
        "print(\"Misclassification rate:\"+str(misclassify_rates.mean()))\n",
        "\n",
        "ious = np.zeros(len(X_test))\n",
        "dices= np.zeros(len(X_test))\n",
        "misclassify_rates = np.zeros(len(X_test))\n",
        "for i in range(len(X_test)):\n",
        "     ious[i] = IoU(open_prediction(preds[i],radius=1),Y_test[i])\n",
        "     dices[i] = Dice(open_prediction(preds[i],radius=1),Y_test[i])\n",
        "     misclassify_rates[i] = misclassification_rate(open_prediction(preds[i],radius=1),Y_test[i])\n",
        "\n",
        "print(\"Mean Dice when morphological opening filter is applied:\"+str(dices.mean()))\n",
        "print(\"Mean IoU when morphological opening filter is applied::\"+str(ious.mean()))\n",
        "print(\"Misclassification rate:\"+str(misclassify_rates.mean()))"
      ],
      "metadata": {
        "id": "JDL3Ikb7tail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above snippet, determine which postprocessing filter is giving better performance with the ground truths than the original model predictions.                                                           "
      ],
      "metadata": {
        "id": "ObEC9yLEfk7-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lung_segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}